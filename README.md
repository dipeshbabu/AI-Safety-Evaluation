# ğŸ›¡ï¸ TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift

TriGuard is a diagnostic toolkit for evaluating the **safety** of image classifiers across three critical dimensions:

- âœ… **Robustness**: Adversarial accuracy and certified verification
- ğŸ§  **Interpretability**: Attribution entropy and drift
- ğŸ“Š **Faithfulness**: Saliency effectiveness under input perturbations

We introduce **Attribution Drift Score (ADS)** and demonstrate how entropy-regularized training improves explanation stability â€” even in models with high adversarial performance.

---

## ğŸš€ Features

- ğŸ“ˆ Multi-axis safety metrics: Accuracy, adversarial error, drift, entropy, SmoothGradÂ², and CROWN-IBP
- ğŸ”¬ Saliency faithfulness via **Deletion/Insertion AUC** curves
- ğŸ” Evaluation with and without entropy-regularized training
- âœ… Supports multiple models (SimpleCNN, ResNet, DenseNet, MobileNetV3)
- ğŸ“¦ Datasets: MNIST, FashionMNIST, CIFAR-10

---

## ğŸ“„ Paper

For methodology, analysis, and results, see the full paper:

[![Paper](https://img.shields.io/badge/Paper-red)](https://arxiv.org/abs/2506.14217)

---

## ğŸ”– Citation

If you use this codebase or find our work helpful, please cite:

```bibtex
@misc{mahato2025triguardtestingmodelsafety,
  title={TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift},
  author={Dipesh Tharu Mahato and Rohan Poudel and Pramod Dhungana},
  year={2025},
  eprint={2506.14217},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2506.14217}
}
```
